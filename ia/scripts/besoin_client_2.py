#!/usr/bin/env python3

"""
Pipeline Machine Learning pour la Pr√©diction du Type de Navire
================================================================

Ce script impl√©mente un pipeline complet de machine learning pour pr√©dire
le type de navire (VesselType) bas√© sur les caract√©ristiques des MMSI.

Utilisation:
    python vessel_prediction.py --train --evaluate
    python vessel_prediction.py --predict --mmsi 209016000
    python vessel_prediction.py --predict --features 8.3,2.4,345.3,570.5,8.5,5.4,...
"""

import argparse
import pandas as pd
import numpy as np
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import joblib
import logging

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Gradient Boosting
import lightgbm as lgb
import xgboost as xgb

warnings.filterwarnings('ignore')

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VesselTypePredictor:
    """
    Classe principale pour la pr√©diction du type de navire.
    
    Cette classe encapsule tout le pipeline de machine learning :
    - Pr√©paration et pr√©traitement des donn√©es
    - Entra√Ænement de multiples mod√®les
    - √âvaluation et s√©lection du meilleur mod√®le
    - Pr√©diction sur de nouvelles donn√©es
    """
    
    def __init__(self):
        self.feature_columns = [
            'mean_draft', 'std_draft', 'mean_distance_travel', 'std_distance_travel',
            'mean_cruise_speed', 'std_cruise_speed', 'mean_dist_coast_travel',
            'std_dist_coast_travel', 'mean_duration_stop', 'std_duration_stop',
            'number_occurence_travel', 'length_width_ratio', 'COG_std',
            'HDG_COG_diff_mean', 'HDG_COG_diff_std', 'Heading_consistency',
            'COG_consistency', 'number_occurence_docked', 'number_occurence_off_shore_docked',
            'onshore_duration_ratio'
        ]
        
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.scaler = None
        self.label_encoder = None
        self.feature_importance = {}
        self.evaluation_results = {}
        
    def vessel_type_to_category(self, vessel_type: float) -> str:
        """
        Convertit les codes VesselType en cat√©gories.
        
        Args:
            vessel_type: Code num√©rique du type de navire
            
        Returns:
            str: Cat√©gorie ('Passenger', 'Cargo', 'Tanker', 'Other')
        """
        if pd.isna(vessel_type):
            return 'Other'
        
        vessel_type = int(vessel_type)
        if 60 <= vessel_type <= 69:
            return 'Passenger'
        elif 70 <= vessel_type <= 79:
            return 'Cargo'
        elif 80 <= vessel_type <= 89:
            return 'Tanker'
        else:
            return 'Other'
    
    def clean_data(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Nettoie les donn√©es en supprimant les valeurs infinies et aberrantes.
        
        Args:
            X: DataFrame des features
            
        Returns:
            pd.DataFrame: DataFrame nettoy√©
        """
        logger.info("Nettoyage des donn√©es...")
        
        X_clean = X.copy()
        
        # Remplacer les valeurs infinies par NaN
        X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
        
        # Statistiques avant nettoyage
        inf_count = np.isinf(X.values).sum()
        nan_count = X.isnull().sum().sum()
        logger.info(f"Avant nettoyage - Valeurs infinies: {inf_count}, NaN: {nan_count}")
        
        # Pour chaque colonne, remplacer les NaN par la m√©diane
        for col in X_clean.columns:
            if X_clean[col].isnull().sum() > 0:
                median_val = X_clean[col].median()
                X_clean[col].fillna(median_val, inplace=True)
                logger.info(f"Colonne {col}: {X_clean[col].isnull().sum()} NaN remplac√©s par m√©diane ({median_val:.2f})")
        
        # D√©tecter et traiter les valeurs aberrantes avec IQR
        for col in X_clean.columns:
            Q1 = X_clean[col].quantile(0.25)
            Q3 = X_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            
            # D√©finir les limites (plus conservateur que 1.5*IQR)
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            
            # Compter les outliers
            outliers = ((X_clean[col] < lower_bound) | (X_clean[col] > upper_bound)).sum()
            
            if outliers > 0:
                # Clip les valeurs extr√™mes
                X_clean[col] = X_clean[col].clip(lower_bound, upper_bound)
                logger.info(f"Colonne {col}: {outliers} valeurs aberrantes clipp√©es")
        
        # V√©rification finale
        inf_count_after = np.isinf(X_clean.values).sum()
        nan_count_after = X_clean.isnull().sum().sum()
        logger.info(f"Apr√®s nettoyage - Valeurs infinies: {inf_count_after}, NaN: {nan_count_after}")
        
        return X_clean
    
    def load_and_prepare_data(self, features_file: str, raw_data_file: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Charge et pr√©pare les donn√©es pour l'entra√Ænement.
        
        Args:
            features_file: Chemin vers le fichier des features calcul√©es
            raw_data_file: Chemin vers le fichier des donn√©es brutes
            
        Returns:
            Tuple[pd.DataFrame, pd.Series]: Features et labels
        """
        logger.info("Chargement des donn√©es...")
        
        # Charger les features
        features_df = pd.read_csv(features_file)
        
        # Charger les donn√©es brutes pour r√©cup√©rer VesselType
        raw_df = pd.read_csv(raw_data_file)
        
        # Merger sur MMSI pour r√©cup√©rer VesselType
        merged_df = features_df.merge(
            raw_df[['MMSI', 'VesselType']].drop_duplicates(),
            on='MMSI',
            how='inner'
        )
        
        logger.info(f"Donn√©es charg√©es: {len(merged_df)} √©chantillons")
        logger.info(f"Distribution des classes:\n{merged_df['VesselType'].value_counts()}")
        
        # Convertir VesselType en cat√©gories
        merged_df['VesselCategory'] = merged_df['VesselType'].apply(self.vessel_type_to_category)
        
        logger.info(f"Distribution des cat√©gories:\n{merged_df['VesselCategory'].value_counts()}")
        
        # Filtrer les classes avec moins de 2 √©chantillons pour la validation crois√©e
        class_counts = merged_df['VesselCategory'].value_counts()
        valid_classes = class_counts[class_counts >= 2].index
        
        if len(valid_classes) < len(class_counts):
            removed_classes = class_counts[class_counts < 2].index
            logger.warning(f"Suppression des cat√©gories avec <2 √©chantillons: {removed_classes.tolist()}")
            merged_df = merged_df[merged_df['VesselCategory'].isin(valid_classes)]
            logger.info(f"Donn√©es apr√®s filtrage: {len(merged_df)} √©chantillons")
            logger.info(f"Nouvelles cat√©gories: {merged_df['VesselCategory'].value_counts()}")
        
        # Pr√©parer X et y
        X = merged_df[self.feature_columns]
        y = merged_df['VesselCategory']
        
        # Nettoyer les donn√©es
        X_clean = self.clean_data(X)
        
        return X_clean, y
    
    def check_multicollinearity(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        V√©rifie la multicolin√©arit√© avec le Variance Inflation Factor (VIF).
        
        Args:
            X: DataFrame des features
            
        Returns:
            pd.DataFrame: DataFrame avec les scores VIF
        """
        logger.info("V√©rification de la multicolin√©arit√© (VIF)...")
        
        try:
            # V√©rifier qu'il n'y a pas de valeurs infinies ou NaN
            if np.isinf(X.values).any() or X.isnull().any().any():
                logger.error("Les donn√©es contiennent encore des valeurs infinies ou NaN")
                return pd.DataFrame()
            
            # Ajouter une constante pour √©viter la singularit√©
            X_with_const = X.copy()
            X_with_const['const'] = 1
            
            # Standardiser les donn√©es pour le calcul VIF
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_with_const)
            X_scaled_df = pd.DataFrame(X_scaled, columns=X_with_const.columns)
            
            # V√©rifier les donn√©es standardis√©es
            if np.isinf(X_scaled).any() or np.isnan(X_scaled).any():
                logger.error("Les donn√©es standardis√©es contiennent des valeurs infinies ou NaN")
                return pd.DataFrame()
            
            vif_data = pd.DataFrame()
            vif_data["Feature"] = X.columns  # Exclure la constante
            vif_scores = []
            
            for i in range(len(X.columns)):  # Exclure la constante
                try:
                    vif_score = variance_inflation_factor(X_scaled_df.values, i)
                    # V√©rifier si le score VIF est valide
                    if np.isfinite(vif_score):
                        vif_scores.append(vif_score)
                    else:
                        vif_scores.append(np.nan)
                        logger.warning(f"VIF non calculable pour {X.columns[i]}")
                except Exception as e:
                    logger.warning(f"Erreur VIF pour {X.columns[i]}: {str(e)}")
                    vif_scores.append(np.nan)
            
            vif_data["VIF"] = vif_scores
            vif_data = vif_data.sort_values('VIF', ascending=False)
            
            # Afficher seulement les VIF valides
            valid_vif = vif_data.dropna()
            if not valid_vif.empty:
                logger.info(f"VIF scores:\n{valid_vif}")
                
                # Signaler les features avec VIF √©lev√© (>10)
                high_vif = valid_vif[valid_vif['VIF'] > 10]
                if not high_vif.empty:
                    logger.warning(f"Features avec VIF √©lev√© (>10):\n{high_vif}")
            else:
                logger.warning("Aucun score VIF valide calcul√©")
            
            return vif_data
            
        except Exception as e:
            logger.error(f"Erreur lors du calcul VIF: {str(e)}")
            return pd.DataFrame()
    
    def preprocess_data(self, X: pd.DataFrame, y: pd.Series, fit_transformers: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Pr√©traite les donn√©es (normalisation, encodage des labels).
        
        Args:
            X: Features
            y: Labels
            fit_transformers: Si True, fit les transformateurs, sinon utilise les existants
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: Features et labels transform√©s
        """
        logger.info("Pr√©traitement des donn√©es...")
        
        if fit_transformers:
            # Normalisation robuste (moins sensible aux outliers)
            self.scaler = RobustScaler()
            X_scaled = self.scaler.fit_transform(X)
            
            # Encodage des labels
            self.label_encoder = LabelEncoder()
            y_encoded = self.label_encoder.fit_transform(y)
            
            logger.info(f"Classes encod√©es: {dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))}")
        else:
            X_scaled = self.scaler.transform(X)
            y_encoded = self.label_encoder.transform(y)
        
        return X_scaled, y_encoded
    
    def create_models(self, n_classes: int) -> Dict[str, Any]:
        """
        Cr√©e les diff√©rents mod√®les √† tester.
        
        Args:
            n_classes: Nombre de classes
            
        Returns:
            Dict[str, Any]: Dictionnaire des mod√®les
        """
        logger.info("Cr√©ation des mod√®les...")
        
        models = {
            'RandomForest': {
                'model': RandomForestClassifier(
                    random_state=42,
                    n_jobs=-1,
                    class_weight='balanced'
                ),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 20, None],
                    'min_samples_split': [2, 5],
                    'min_samples_leaf': [1, 2]
                }
            },
            
            'LightGBM': {
                'model': lgb.LGBMClassifier(
                    random_state=42,
                    n_jobs=-1,
                    class_weight='balanced',
                    verbosity=-1
                ),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [5, 10, 15],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'num_leaves': [31, 50, 100]
                }
            },
            
            'XGBoost': {
                'model': xgb.XGBClassifier(
                    random_state=42,
                    n_jobs=-1,
                    eval_metric='mlogloss'
                ),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [3, 6, 10],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'subsample': [0.8, 1.0]
                }
            },
        }
        
        return models
    
    def train_and_evaluate_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Dict]:
        """
        Entra√Æne et √©value tous les mod√®les avec GridSearchCV.
        
        Args:
            X: Features normalis√©es
            y: Labels encod√©s
            
        Returns:
            Dict[str, Dict]: R√©sultats d'√©valuation pour chaque mod√®le
        """
        logger.info("Entra√Ænement et √©valuation des mod√®les...")
        
        # V√©rifier le nombre minimum d'√©chantillons par classe
        unique_classes, class_counts = np.unique(y, return_counts=True)
        min_class_count = np.min(class_counts)
        
        logger.info(f"Nombre minimum d'√©chantillons par classe: {min_class_count}")
        
        # Ajuster le nombre de folds si n√©cessaire
        n_splits = min(5, min_class_count)
        if n_splits < 2:
            logger.error("Pas assez d'√©chantillons pour la validation crois√©e")
            raise ValueError("Au moins 2 √©chantillons par classe sont n√©cessaires")
        
        logger.info(f"Utilisation de {n_splits} folds pour la validation crois√©e")
        
        # Split train/test avec stratification
        test_size = max(0.1, min(0.3, 1.0 / min_class_count))
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # SMOTE pour √©quilibrer les classes sur le train set uniquement
        unique_train, train_counts = np.unique(y_train, return_counts=True)
        min_train_count = np.min(train_counts)
        
        if min_train_count >= 2:
            try:
                smote = SMOTE(random_state=42, k_neighbors=min(5, min_train_count-1))
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                logger.info(f"Apr√®s SMOTE - Distribution: {np.bincount(y_train_balanced)}")
            except Exception as e:
                logger.warning(f"SMOTE √©chou√©: {str(e)} - Utilisation des donn√©es non √©quilibr√©es")
                X_train_balanced, y_train_balanced = X_train, y_train
        else:
            logger.warning("SMOTE non applicable - pas assez d'√©chantillons par classe")
            X_train_balanced, y_train_balanced = X_train, y_train
        
        models = self.create_models(len(np.unique(y)))
        results = {}
        
        # Cross-validation stratifi√©e adapt√©e
        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        for name, model_config in models.items():
            logger.info(f"Entra√Ænement du mod√®le: {name}")
            
            try:
                # Adapter les param√®tres pour les petits datasets
                adapted_params = self.adapt_params_for_small_dataset(
                    model_config['params'], len(X_train_balanced)
                )
                
                # GridSearchCV avec gestion d'erreurs am√©lior√©e
                grid_search = GridSearchCV(
                    model_config['model'],
                    adapted_params,
                    cv=cv,
                    scoring='f1_weighted',
                    n_jobs=1,
                    verbose=0,
                    error_score='raise'  # Pour d√©boguer les erreurs
                )
                
                # Fit sur les donn√©es √©quilibr√©es
                grid_search.fit(X_train_balanced, y_train_balanced)
                
                # Meilleur mod√®le
                best_model = grid_search.best_estimator_
                self.models[name] = best_model
                
                # Pr√©dictions sur le test set
                y_pred = best_model.predict(X_test)
                y_pred_proba = best_model.predict_proba(X_test)
                
                # M√©triques
                accuracy = accuracy_score(y_test, y_pred)
                precision, recall, f1, _ = precision_recall_fscore_support(
                    y_test, y_pred, average='weighted', zero_division=0
                )
                
                # ROC AUC pour classification multi-classe
                try:
                    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')
                except Exception as e:
                    logger.warning(f"Impossible de calculer ROC AUC pour {name}: {str(e)}")
                    roc_auc = None
                
                results[name] = {
                    'best_params': grid_search.best_params_,
                    'best_score': grid_search.best_score_,
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'roc_auc': roc_auc,
                    'classification_report': classification_report(y_test, y_pred, zero_division=0),
                    'confusion_matrix': confusion_matrix(y_test, y_pred)
                }
                
                logger.info(f"{name} - F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}")
                
                # Feature importance si disponible
                if hasattr(best_model, 'feature_importances_'):
                    self.feature_importance[name] = best_model.feature_importances_
                
            except Exception as e:
                logger.error(f"Erreur lors de l'entra√Ænement de {name}: {str(e)}")
                results[name] = {'error': str(e)}
        
        self.evaluation_results = results
        
        # S√©lection du meilleur mod√®le bas√© sur le F1 score
        best_f1 = 0
        for name, result in results.items():
            if 'f1_score' in result and result['f1_score'] > best_f1:
                best_f1 = result['f1_score']
                self.best_model_name = name
                self.best_model = self.models[name]
        
        if self.best_model_name:
            logger.info(f"Meilleur mod√®le: {self.best_model_name} (F1: {best_f1:.4f})")
        else:
            logger.warning("Aucun mod√®le valide trouv√©")
        
        return results
    
    def adapt_params_for_small_dataset(self, params: Dict, n_samples: int) -> Dict:
        """
        Adapte les hyperparam√®tres pour les petits datasets.
        
        Args:
            params: Param√®tres originaux
            n_samples: Nombre d'√©chantillons
            
        Returns:
            Dict: Param√®tres adapt√©s
        """
        adapted_params = params.copy()
        
        # R√©duire les param√®tres pour √©viter l'overfitting sur petits datasets
        if n_samples < 200:
            # R√©duire n_estimators
            if 'n_estimators' in adapted_params:
                adapted_params['n_estimators'] = [50, 100, 150]
            
            # R√©duire max_depth
            if 'max_depth' in adapted_params:
                adapted_params['max_depth'] = [3, 5, 10]
            
            # R√©duire iterations pour CatBoost
            if 'iterations' in adapted_params:
                adapted_params['iterations'] = [50, 100, 150]
                
            # R√©duire depth pour CatBoost
            if 'depth' in adapted_params:
                adapted_params['depth'] = [3, 4, 6]
                
            # R√©duire num_leaves pour LightGBM
            if 'num_leaves' in adapted_params:
                adapted_params['num_leaves'] = [15, 31, 50]
        
        return adapted_params
    
    def save_model(self, filepath: str):
        """Sauvegarde le mod√®le et les transformateurs."""
        model_data = {
            'best_model': self.best_model,
            'best_model_name': self.best_model_name,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_columns': self.feature_columns,
            'evaluation_results': self.evaluation_results
        }
        
        joblib.dump(model_data, filepath)
        logger.info(f"Mod√®le sauvegard√©: {filepath}")
    
    def load_model(self, filepath: str):
        """Charge un mod√®le sauvegard√©."""
        model_data = joblib.load(filepath)
        
        self.best_model = model_data['best_model']
        self.best_model_name = model_data['best_model_name']
        self.scaler = model_data['scaler']
        self.label_encoder = model_data['label_encoder']
        self.feature_columns = model_data['feature_columns']
        self.evaluation_results = model_data.get('evaluation_results', {})
        
        logger.info(f"Mod√®le charg√©: {filepath}")
    
    def predict(self, features: np.ndarray) -> Tuple[str, float]:
        """
        Pr√©dit le type de navire pour de nouvelles features.
        
        Args:
            features: Array des features (1D ou 2D)
            
        Returns:
            Tuple[str, float]: Classe pr√©dite et probabilit√© maximale
        """
        if self.best_model is None:
            raise ValueError("Aucun mod√®le entra√Æn√©. Utilisez train() d'abord.")
        
        # Reshaper si n√©cessaire
        if features.ndim == 1:
            features = features.reshape(1, -1)
        
        # Normaliser
        features_scaled = self.scaler.transform(features)
        
        # Pr√©dire
        prediction = self.best_model.predict(features_scaled)
        probabilities = self.best_model.predict_proba(features_scaled)
        
        # D√©coder le label
        vessel_type = self.label_encoder.inverse_transform(prediction)[0]
        max_probability = np.max(probabilities[0])
        
        return vessel_type, max_probability
    
    def predict_from_mmsi(self, mmsi: int, features_file: str) -> Tuple[str, float]:
        """
        Pr√©dit le type de navire √† partir d'un MMSI.
        
        Args:
            mmsi: MMSI du navire
            features_file: Fichier des features
            
        Returns:
            Tuple[str, float]: Classe pr√©dite et probabilit√©
        """
        features_df = pd.read_csv(features_file)
        
        # Trouver le navire
        vessel_data = features_df[features_df['MMSI'] == mmsi]
        
        if vessel_data.empty:
            raise ValueError(f"MMSI {mmsi} non trouv√© dans les donn√©es")
        
        # Extraire les features
        features = vessel_data[self.feature_columns].values
        
        # Nettoyer les features
        features_df_clean = self.clean_data(pd.DataFrame(features, columns=self.feature_columns))
        features_clean = features_df_clean.values
        
        return self.predict(features_clean)
    
    def print_evaluation_summary(self):
        """Affiche un r√©sum√© de l'√©valuation de tous les mod√®les."""
        if not self.evaluation_results:
            logger.warning("Aucun r√©sultat d'√©valuation disponible")
            return
        
        print("\n" + "="*70)
        print("R√âSUM√â DE L'√âVALUATION DES MOD√àLES")
        print("="*70)
        
        for name, results in self.evaluation_results.items():
            if 'error' in results:
                print(f"\n{name}: ERREUR - {results['error']}")
                continue
                
            print(f"\n{name}:")
            print(f"  Meilleurs param√®tres: {results['best_params']}")
            print(f"  Score CV: {results['best_score']:.4f}")
            print(f"  Accuracy: {results['accuracy']:.4f}")
            print(f"  Precision: {results['precision']:.4f}")
            print(f"  Recall: {results['recall']:.4f}")
            print(f"  F1 Score: {results['f1_score']:.4f}")
            if results['roc_auc']:
                print(f"  ROC AUC: {results['roc_auc']:.4f}")
        
        print(f"\nüèÜ MEILLEUR MOD√àLE: {self.best_model_name}")
        print("="*70)
        
        # Afficher la classification d√©taill√©e du meilleur mod√®le
        if self.best_model_name and self.best_model_name in self.evaluation_results:
            print(f"\nRapport de classification d√©taill√© - {self.best_model_name}:")
            print(self.evaluation_results[self.best_model_name]['classification_report'])

def main():
    """Fonction principale avec interface en ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Pipeline ML pour la pr√©diction du type de navire"
    )
    
    # Arguments principaux
    parser.add_argument('--train', action='store_true', 
                       help='Entra√Æner les mod√®les')
    parser.add_argument('--evaluate', action='store_true',
                       help='√âvaluer les mod√®les')
    parser.add_argument('--predict', action='store_true',
                       help='Faire une pr√©diction')
    
    # Fichiers de donn√©es
    parser.add_argument('--features-file', default='../data/features_completes.csv',
                       help='Fichier des features (d√©faut: ../data/features_completes.csv)')
    parser.add_argument('--raw-data-file', default='../data/export_IA_final.csv',
                       help='Fichier des donn√©es brutes (../data/export_IA_final.csv)')
    parser.add_argument('--model-file', default='../data//vessel_model.pkl',
                       help='Fichier du mod√®le (d√©faut: ../data/vessel_model.pkl)')
    
    # Options de pr√©diction
    parser.add_argument('--mmsi', type=int,
                       help='MMSI pour la pr√©diction')
    parser.add_argument('--features', type=str,
                       help='Features s√©par√©es par des virgules pour la pr√©diction')
    
    args = parser.parse_args()
    
    # Initialiser le pr√©dicteur
    predictor = VesselTypePredictor()
    
    try:
        if args.train or args.evaluate:
            # Charger et pr√©parer les donn√©es
            X, y = predictor.load_and_prepare_data(args.features_file, args.raw_data_file)
            
            # V√©rifier la multicolin√©arit√©
            vif_scores = predictor.check_multicollinearity(X)
            
            # Pr√©traitement
            X_processed, y_processed = predictor.preprocess_data(X, y)
            
            # Entra√Ænement et √©valuation
            results = predictor.train_and_evaluate_models(X_processed, y_processed)
            
            # Sauvegarder le mod√®le
            predictor.save_model(args.model_file)
            
            if args.evaluate:
                predictor.print_evaluation_summary()
        
        elif args.predict:
            # Charger le mod√®le
            if Path(args.model_file).exists():
                predictor.load_model(args.model_file)
            else:
                raise FileNotFoundError(f"Mod√®le non trouv√©: {args.model_file}")
            
            if args.mmsi:
                # Pr√©diction √† partir d'un MMSI
                vessel_type, probability = predictor.predict_from_mmsi(
                    args.mmsi, args.features_file
                )
                print(f"\nPr√©diction pour MMSI {args.mmsi}:")
                print(f"Type de navire: {vessel_type}")
                print(f"Probabilit√©: {probability:.4f}")
                
            elif args.features:
                # Pr√©diction √† partir de features
                features_list = [float(x.strip()) for x in args.features.split(',')]
                
                if len(features_list) != len(predictor.feature_columns):
                    raise ValueError(f"Nombre de features incorrect. Attendu: {len(predictor.feature_columns)}, Re√ßu: {len(features_list)}")
                
                features_array = np.array(features_list)
                vessel_type, probability = predictor.predict(features_array)
                
                print(f"\nPr√©diction:")
                print(f"Type de navire: {vessel_type}")
                print(f"Probabilit√©: {probability:.4f}")
            
            else:
                print("Erreur: Sp√©cifiez --mmsi ou --features pour la pr√©diction")
        
        else:
            parser.print_help()
    
    except Exception as e:
        logger.error(f"Erreur: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
